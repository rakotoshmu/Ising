\documentclass[a4paper,11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
% \usepackage{fullpage}
\usepackage{amsmath, amsfonts, amssymb, amsthm}
% \usepackage{mathabx}
% \usepackage{bbm}
\usepackage{stmaryrd}
\usepackage{graphicx}
% \usepackage{enumerate}

\usepackage{float}

\input{preamble_Macro}

\title{Projet de simulations aléatoires : Modèles d'Ising}
\author{Aurélien Enfroy, Shmuel Rakotonirina{-}-Ricquebourg}

\begin{document}
\maketitle
\tableofcontents

\section{Introduction}

Ce document rend compte des expériences sur les modèles d'Ising sur un réseau carré, avec pour objectifs principaux d'observer la transition de phase à la température critique, en fonction de la taille du réseau.

Nous avons fait le choix, pour s'approcher le mieux possible d'un réseau infini, de prendre des conditions aux bords périodiques (par exemple, les points du côté droit du carré sont voisins du côté gauche).

Pour l'observation de la température critique (réseau de grande taille), nous avons implémenté l'algorithme de Metropolis-Hastings (proposition du mouvement détaillé en section \ref{sec:MH}), l'échantilloneur de Gibbs (section \ref{sec:gibbs}) et deux méthodes de couplage par le passé, chacune basée sur une des méthodes précédentes (section \ref{sec:coupling}).

En très petite taille (4 ou 9 sommets sur le graphe), nous avons aussi implémenté la méthode naïve (section \ref{sec:naive}) pour observer à quel point les états $\pm(1,\hdots,1)$ étaient probable (et donc "résistants" aux transitions de Metropolis-Hastings).

Enfin, la section \ref{sec:check} présente la magnétisation et le test qui en découle pour vérifier que les algorithmes fonctionnent.

\section{Choix d'implémentation}\label{sec:implementation}

On rappelle la définition du modèle d'Ising sur un réseau carré :
\begin{definition}
On fixe $C$ le réseau carré de dimension 2 de taille $N^2$. Le modèle d'Ising est la distribution sur l'espace d'état $\set{\pm 1}^C$ dont la loi est donnée par
$$\forall x \in \set{\pm 1}^C, \pi(x) = \frac{1}{Z_T} \exp \left(\left( \somme{u\sim v}{} J_{u,v} x_{u} x_{v} + \somme{u}{} h_{u} x_{u} \right)/T\right)$$
où $T>0$ est appelée la température, $Z_T$ est une constante de normalisation, $J_{u,v}$ est la force d'interaction entre $u$ et $v$ et $h_{u}$ est le champ magnétique extérieur en $u$.
\end{definition}

Pour l'implémentation, on remarque qu'il n'y a pas besoin du paramètre $T$, qu'on peut inclure dans $J$ et $h$. On représente alors ces paramètres en prenant $x \in \mathcal M_{N,N}(\set{\pm 1})$, $\tilde h = h/T \in \mathcal M_{N,N}(\mathbb R)$ et $J$ comme une matrice à trois entrées $\tilde J \in \mathcal M_{N,N,2}(\mathbb R)$ où
$$\tilde J_{i,j,1} = J_{(i,j),(i+1,j)}/T \text{ et } \tilde J_{i,j,2} = J_{(i,j),(i,j+1)}/T$$
où $i+1$ et $j+1$ sont à comprendre modulo $N$ pour obtenir des conditions aux bords périodiques.

\section{Simulation par Metropolis-Hastings}\label{sec:MH}

Nous avons implémenté l'algorithme de Metropolis-Hastings en utilisant la fonction de rejet de Metropolis-Hastings. Pour le choix du noyau instrumental $Q$, nous avons choisi de donner à une coordonnée aléatoire (uniforme) une valeur aléatoire (uniforme), ce qui permet d'avoir un noyau symétrique irréductible apériodique (probabilité strictement positive de rester sur place).

Avec ce noyau instrumental, on peut réduire les calculs effectués : si $x$ est un état et $y$ l'état proposé à partir de $x$, alors
\begin{itemize}
	\item soit $y = x$, auquel cas on peut considérer qu'on accepte le mouvement,
	\item soit $y \neq x$, auquel cas ils ne diffèrent que d'au plus une coordonnée (celle tirée uniformément). Si on note $u$ cette coordonnée et $s$ la nouvelle valeur, alors $s = y_u = - x_u$ et, en considérant que $T$ a été pris en compte dans $J$ et $h$,
	\begin{align*}
	\frac{\pi(y)}{\pi(x)}
	&= \frac{\exp \left( \somme{v \sim u}{} J_{u,v} s x_{v} + h_{u} s \right)}{\exp \left( \somme{v \sim u}{} J_{u,v} (-s) x_{v} + h_{u} (-s) \right)}\\
	&= \frac{e^{s V_u(x)}}{e^{-s V_u(x)}}\\
	&= e^{2 s V_u(x)}
	\end{align*}
	où on a noté $V_u(x) \doteq \somme{v \sim u}{} J_{u,v} x_v + h_u$.
\end{itemize}

\section{Simulation par l'échantilloneur de Gibbs}\label{sec:gibbs}

En reprenant les notations du cours, on a pour $u \in C$ et $x \in \set{\pm 1}^C$, en considérant que $J$ et $h$ dépendent de $T$,
\begin{align*}
\pi_u(x_u \mid x^u)
&= \frac{\pi(x_u,x^u)}{\pi(1,x^u) + \pi(-1,x^u)}\\
&= \frac{e^{x_u V_u(x)}}{e^{V_u(x)} + e^{-V_u(x)}}
\end{align*}
où on note encore $V_u(x) \doteq \somme{v \sim u}{} J_{u,v} x_v + h_u$. Donc $\pi_u(\cdot \mid x^u) = \mathcal B(\frac{e^{V_u(x)}}{e^{V_u(x)} + e^{-V_u(x)}}) = B(\frac{1}{1 + e^{-2V_u(x)}})$

\section{Couplage par le passé}\label{sec:coupling}

Pour le couplage par le passé, deux fonctions d'actualisation sont possibles. 
\begin{itemize}
	\item celle vue en cours, issue de l'échantillonneur de Gibbs (par balayage séquentiel)
	\item celle de \cite{propp1998coupling}, issue de l'algorithme de Metropolis-Hastings (i.e. même transition qu'en section \ref{sec:MH}).
\end{itemize}
Dans les deux cas, la méthode n'a de garantie théorique que si $J$ est constant et $h$ nul. On supposera donc cela vérifié.

L'algorithme consiste à répéter les algorithmes vers le futur jusqu'à un temps donné et, si il n'y a pas eu coalition, ajouter des nouvelles actualisations au début du processus, ce qui demande de recommencer du début. Pour gagner du temps, plutôt que d'ajouter une actualisation à chaque fois, on double le nombre d'actualisations.

Pour les deux méthodes (Metropolis-Hastings et échantillonneur de Gibbs), les choses se passent bien au-delà de la température critique. Cependant, en dessous de la température critique, les états $\pm(1,\hdots,1)$ ont tendance à rester inchangés par les fonctions d'actualisation précédentes : dans les deux cas, la probabilité de réussir à changer un point $x_u$ égaux à ses quatre voisins est très faible : si $J>0$ (toujours supposé constant), alors $\exp(-8 J/T)$.

En conséquence, l'algorithme met extrêmement longtemps à terminer (nous n'avons pas vu terminer, SciLab se fermait avant). En observant l'évolution des chaînes de Markov (parties de $(1,\hdots,1)$ et de $(-1,\hdots,-1)$), on voit que souvent les deux chaînes reviennent à leur état initial, ce qui signifie que l'algorithme pourrait bien continuer de boucler ainsi très longtemps.

\section{Simulation naïve en petite taille}\label{sec:naive}

En petite taille, on peut faire une simulation naïve. Pour cela, on numérote les $2^{N^2}$ états (dans l'ordre lexicographique en lisant les matrices colonne par colonne), on calcule exactement la loi et on choisit un numéro en utilisant cette loi et une loi uniforme sur $[0,1]$.

On a donc une représentation (dans un vecteur \texttt{p} dans le code SciLab) de toutes les probabilités de tous les états, et des fonctions (\texttt{etat2num} et \texttt{num2etat}) qui associent une matrice $x \in \set{\pm 1}^C$ à sa numérotation $m \in \llbracket 1,2^{N^2} \rrbracket$ ou une numérotation à sa matrice.

Le calcul des probabilités exactes permet de tracer, en fonction de $T$, la probabilité d'un état donné. La figure \ref{fig:tc_naive_N4} trace la probabilité de l'état $(1,\hdots,1)$ (qui est celle de $(-1,\hdots,-1)$).
\begin{figure}[H]
	\label{fig:tc_naive_N4}
	\includegraphics[width=0.8\textwidth]{temperature_critique_naive_N4.png}
	\caption{$\pi(1,\hdots,1)$ en fonction de $T$ pour $N = 4$}
\end{figure}
On observe une décroissance assez lente autour de la température critique théorique $T_c \simeq 2.269$. La théorie dit que quand $N \rightarrow +\infty$, cette décroissance devient de plus en plus forte et de plus en plus concentrée autour de $T_c$. En effet, les spins différents ont plus de chance de coexister en haute température, les états avec un spin majoritaire sont donc moins probables.

\section{Vérification des algorithmes}\label{sec:check}

Pour vérifier que les algorithmes MCMC fonctionnent, on vérifie l'ergodicité sur la fonction $M(x) = \frac{1}{N^2} \sum_u x_u$. $M$ est appelée la magnétisation. Dans le cas où $h = 0$, on a $\pi(-x) = \pi(x)$ et $M(-x) = -M(x)$ donc si $X \sim \pi$, alors $\mathbb E(M(X)) = 0$. Si la chaîne $(X^{(n)})_n$ est ergodique, on aura donc la convergence presque sûre
$$\frac{1}{n} \sum_i M(X^{(i)}) \xrightarrow[n \rightarrow +\infty]{} 0$$
En pratique, à basse température, les algorithmes ont tendance à rester longtemps dans les états $(1,\hdots,1)$ et $(-1,\hdots,-1)$ (qui sont très probables à basse température) donc il faudrait attendre un temps très long pour voir cette convergence. On se place donc à haute température.

Pour l'algorithme exact (couplage par le passé), on simule de manière indépendante un échantillon $(X^{(n)})_n$ et on vérifie que la loi des grands nombres s'applique :
$$\frac{1}{n} \sum_i M(X^{(i)}) \xrightarrow[n \rightarrow +\infty]{} 0.$$

On obtient les résultats de la figure \ref{fig:ergodic}.
\begin{figure}[H]
	\label{fig:ergodic}
	\includegraphics[width=0.8\textwidth]{ergodicite_magnetisation.png}
	\caption{Vérification de l'ergodicité et de la loi des grands nombres sur la magnétisation}
\end{figure}

\nocite{*}
\bibliographystyle{plain}
\bibliography{biblio}

\end{document}